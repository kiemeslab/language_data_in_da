{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d2faf8e",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "TU Darmstadt<br/>\n",
    "Summer Term 2023<br/>\n",
    "Linguistic Data for Discourse Analysis<br/>\n",
    "Carina Kiemes M.A.<br/><br/>\n",
    "<center><h1>Assignment I</h1></center>\n",
    "<br/><div style=\"text-align: right\"><b>Submission deadline:</b> 31st of May 11.59 pm</div>\n",
    "<div style=\"text-align: right\">Please submit the notebook as .ipynb file in the moodle course</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26de96c",
   "metadata": {},
   "source": [
    "### 1\\. Task: Regular Expressions\n",
    "\n",
    "<br/><br/>Remember to compare your results carefully with the given goal. To test your results, you can copy the examples to https://regex101.com/ and work out the matching RegEx patterns with its help.\n",
    "\n",
    "<br/><br/>\n",
    "**1\\. Describe in your own words what Regular Expressions are and why the are useful to work with. (max. 100 words) (3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cbbf0d",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145f73c",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "**2\\. How can you match the following dates with one RegEx? (2 points)<br/><br/>\n",
    "Friday, August 28, 1970<br/>\n",
    "Wednesday, February 28, 1973<br/>\n",
    "Friday, November 5, 1976<br/>\n",
    "Tuesday, January 4, 1977<br/>\n",
    "Friday, June 22, 1979<br/>\n",
    "Thursday, July 29, 1982<br/>\n",
    "Wednesday, April 25, 1984<br/>\n",
    "Wednesday, September 24, 2003<br/>\n",
    "Monday, July 12, 2010<br/>\n",
    "Tuesday, March 24, 2015<br/>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb3d277",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cbbd34",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "**3\\. How can you match the three following parts in our nexis toy corpus?<br/>\n",
    "Remember that due to the structure of the xml files, you also have to consider tab spaces in your RegEx pattern.\n",
    "To rebuild the structure with the tab spaces I recommend using https://regexr.com/. (6 points)<br/><br/>**\n",
    "```\n",
    "<corpus>\n",
    "    <document>\n",
    "        <metadata>\n",
    "            <source>Sunday Mail</source>\n",
    "            <publication_type>sunday newspaper</publication_type>\n",
    "            <distribution>national</distribution>\n",
    "            <date>16. December 2019</date>\n",
    "            <day>16</day>\n",
    "            <month>12</month>\n",
    "            <year>2019</year>\n",
    "```\n",
    "\n",
    "```\n",
    "<corpus>\n",
    "    <document>\n",
    "        <metadata>\n",
    "            <source>The Times</source>\n",
    "            <publication_type>daily newspaper</publication_type>\n",
    "            <distribution>national</distribution>\n",
    "            <date>19. March 2001</date>\n",
    "            <day>19</day>\n",
    "            <month>03</month>\n",
    "            <year>2001</year>\n",
    "            <author>Garry Herbert</author>\n",
    "            <section>Sport</section>\n",
    "```\n",
    "```\n",
    "<corpus>\n",
    "    <document>\n",
    "        <metadata>\n",
    "          <source>The Guardian</source>\n",
    "          <publication_type>daily newspaper</publication_type>\n",
    "          <distribution>national</distribution>\n",
    "          <date>1. May 2007</date>\n",
    "          <day>01</day>\n",
    "          <month>05</month>\n",
    "          <year>2007</year>\n",
    "          <author>Alice Wignall</author>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1eb00b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b931e20",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "### 2. Task: Python basics\n",
    "\n",
    "In this task, we will use Python to make a first attempt at tokenization and calculating the token frequencies.\n",
    "\n",
    "Most of the code is already given, but some steps are not yet complete. So whenever you read <code><mark># please add code here</mark></code> complete the code to match the task description of each step. You will find help and explanations of the given code in the code comments and in the debug info. Use the notebook of the course session and the online materials of the course [Applied Language Technology](https://applied-language-technology.mooc.fi/html/notebooks/part_i/02_getting_started_with_python.html) as support and reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b896e",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "**1\\. Read the corpus file and discard the lines containing metadata. Instead of using \\s to refer to whitespace characters, write down the tab spaces in your RegEx. (1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d2d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Nexis toy corpus and iterate over each line of the opened file.\n",
    "# Only the lines in the text list should be kept.\n",
    "with open('Nexis_corpus_assignment_1.xml', encoding='utf8') as corpus:\n",
    "    text = []\n",
    "    for line in corpus:\n",
    "        if line.startswith(\"# please add code here\"):\n",
    "            text.append(line)\n",
    "            \n",
    "# ******************** debug info ********************\n",
    "gold = (f\"\\t\\t\\t<p>Mary Lou McDonald grew up in a republican household in Dublin \"\n",
    "        f\"to a backdrop of the Troubles in Northern Ireland. \\\"My family's connections \"\n",
    "        f\"with the IRA would have been in the 1920s,\\\" she says. But it was the death \"\n",
    "        f\"of the hunger strikers in the Maze prison in 1981, when she was 12, that \"\n",
    "        f\"prompted her own political awakening.</p>\\n\")\n",
    "if not text or not text[0] == gold:\n",
    "    print(f\"Whoops, that's not it. The first line should look like this:\"\n",
    "          f\"\\n\\n\\\"\\t{gold.strip()}\\\"\\n\\nPlease try again.\")\n",
    "    raise ValueError(text)\n",
    "else:\n",
    "    print(\"Well done! You have filtered out successfully the metadata:\")\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5befbb33",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "**2\\. Clean the remaining text from markup (2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over each paragraph that we stored in the 'text' variable,\n",
    "# strip its markup and store the result in the 'cleaned_text' list\n",
    "cleaned_text = []\n",
    "for paragraph in text:\n",
    "    # first strip the leading and trailing whitespace\n",
    "    paragraph_stripped = paragraph.strip()\n",
    "    # now you have to get rid of the markup of the paragraphs (the opening as well as the closing element)\n",
    "    paragraph_without_markup = paragraph_stripped.replace(\"<p>\", \"\").replace(# please add code here)\n",
    "    # Use the append()-method to append your list of cleaned_text with the paragraphs_without_markup\n",
    "    # please add code here\n",
    "\n",
    "\n",
    "# ******************** debug info ********************\n",
    "gold = (f\"Mary Lou McDonald grew up in a republican household in Dublin to a backdrop \"\n",
    "        f\"of the Troubles in Northern Ireland. \\\"My family's connections with the IRA would \"\n",
    "        f\"have been in the 1920s,\\\" she says. But it was the death of the hunger strikers in \"\n",
    "        f\"the Maze prison in 1981, when she was 12, that prompted her own political awakening.\")\n",
    "if not cleaned_text or not cleaned_text[0] == gold:\n",
    "    print(f\"Hm, that did not work. Strip the remaining markup and remember to append the \"\n",
    "          f\"cleaned paragraphs to the 'cleaned_text' variable. Without the markup, the first \"\n",
    "          f\"paragraph would be:\\n\\n\\\"{gold}\\\"\\n\")\n",
    "    raise ValueError(cleaned_text)\n",
    "else:\n",
    "    print(\"Good job! You have removed all of the remaining markup:\")\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b407e5e",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "**3\\. Now join the paragraphs back into a single text string. Consult the chapter [Manipulating text using Python](https://applied-language-technology.mooc.fi/html/notebooks/part_ii/01_basic_text_processing.html#manipulating-text-using-python) of the <i>Applied Language Technology</i> course on how to do that. (1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the 'cleaned_text' list into a single string 'joined_text'\n",
    "# you can use the join()-method to do so\n",
    "joined_text = # please add code here\n",
    "\n",
    "\n",
    "# ******************** debug info ********************\n",
    "if not type(joined_text) == str:\n",
    "    print(f\"Please join the cleaned paragraph list into a single text string!\"\n",
    "          f\"Think about what is seperating the string of text in our data.\"\n",
    "          f\"The newline break seems to be a good indicator for string seperation.\")\n",
    "    raise ValueError(joined_text)\n",
    "else:\n",
    "    print(\"Very good! Now we are ready to prepare our corpus for tokenization.\\n\")\n",
    "    print(joined_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f226311",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "**4\\. Append whitespace to punctuation characters in order to get better whitespace tokenization results. Does that work in all cases? Please describe an ambiguous case in a few words, where the result of this step might not be desired. (3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b11a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append whitespace to punctuation characters and\n",
    "# store the result as 'tokenization_prepared'\n",
    "tokenization_prepared = joined_text.replace(\".\", \" .\").replace(\"?\", \" ?\")# please add code here\n",
    "tokenization_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e879e473",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "**5\\. Now tokenize the text at whitespace characters. How many tokens do you count? (1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text at whitespace and store the tokens as a list,\n",
    "# then calculate the length of the list\n",
    "tokens = tokenization_prepared.split()\n",
    "token_count = # please add code here\n",
    "\n",
    "# ******************** debug info ********************\n",
    "if not type(token_count) == int:\n",
    "    print(f\"Please provide the token count as a number. \"\n",
    "          f\"The len()-method is an easy way to do this.\")\n",
    "    raise ValueError(token_count)\n",
    "else:\n",
    "    print(f\"{token_count} tokens? That's some.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfbd38e",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "**6\\. Let's finally calculate the token frequencies. How often does a token occur in the corpus and which is the most frequent? (1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2de44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# calculate the type:token distribution by counting the token occurrences;\n",
    "# sort the result\n",
    "counter = # please add code here\n",
    "\n",
    "\n",
    "# ******************** debug info ********************\n",
    "if not type(counter) == list:\n",
    "    print(f\"Hint: Use the 'Counter' datatype and its 'most_common' method \"\n",
    "          f\"from the built-in 'collections' module.\")\n",
    "    raise ValueError(counter)\n",
    "else:\n",
    "    print(f\"Very well done! \\\"{counter[0][0]}\\\" with {counter[0][1]} occurrences \"\n",
    "          f\"seems to be the most frequent token in our corpus.\")\n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97339193",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "### 3. Task: Tagging with spaCy\n",
    "\n",
    "To be honest: The linguistic processing of our last task has some room for improvement. In order to enhance our data we can use the python library [<i>spaCy</i>](https://spacy.io/), one of the best and most popular nlp libraries out there - it's fast, it's rather easy and straight forward to use and its documentation sets a gold standard for nlp tools.\n",
    "\n",
    "Also - despite hopefully fulfilling its didactic purpose - our way of parsing the XML output in the last task was a bit clumsy and not very robust. This time, we'll use Python's own XML parsing module <i>ElementTree</i>. We're going to read in our tiny newspaper corpus, annotate it with pos tags, count tokens and token frequencies and lastly, we will arrange our tagged corpus into a form, that we can import in the linguistic concordancer and analysis tool [<i>AntConc</i>](https://www.laurenceanthony.net/software/antconc/).\n",
    "\n",
    "As always: Whenever you read <code><mark># please add code here</mark></code> please complete the code to match the task description and the expected output. Detailled help and explanations are given in the code comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd990884",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "**1\\. First things first: Import the modules and external libraries that we're going to use in this task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd9416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aa18b9",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "**2\\. Read in the XML corpus of our first task and collect its text by using the <i>ElementTree</i> module.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e000ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElementTree parses the XML and builds up an internal tree structure\n",
    "# of the XMLs elements - hence the name ;)\n",
    "corpus_tree = ET.parse(\"Nexis_corpus_assignment_1.xml\")\n",
    "\n",
    "# the findall method accepts XPath expressions for finding interesting XML elements -\n",
    "# in our case all <text> elements of the corpus\n",
    "text_elements = corpus_tree.findall(\"document/text\")\n",
    "\n",
    "# with the <text> elements in a list, we can extract their\n",
    "# (and their children elements) actual text with itertext()\n",
    "article_texts = []\n",
    "for text_element in text_elements:\n",
    "    # itertext() loops over every sub element and returns its text\n",
    "    paragraphs = text_element.itertext()\n",
    "    # strip possible leading or trailing whitespace of each paragraph,\n",
    "    # then join them into a single string\n",
    "    article_text = \" \".join([paragraph.strip() for paragraph in paragraphs])\n",
    "    article_texts.append(article_text.strip())\n",
    "\n",
    "# now the list 'article_texts' contains three strings -\n",
    "# the concatenated paragraphs of each article\n",
    "article_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5cea86",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "**3\\. Great - now let's tag the text with spaCy! (1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1acb9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we have to load the appropriate spaCy model\n",
    "nlp = # please add code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b846d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nlp.pipe() on a sequence of texts is much more efficient\n",
    "# than calling nlp() on every single text separately\n",
    "docs = list(nlp.pipe(article_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdeecf2",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "**4\\. How many tokens does spaCy count?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd7fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = sum([len(doc) for doc in docs])\n",
    "token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e0988",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "**5\\. AntConc needs its data input of the form {token}_{pos tag}. Complete the code, so the output matches the content of the files in the directory 'Output_for_AntConc'. (2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8de69b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first loop over the annotated docs\n",
    "for doc in docs:\n",
    "    tagged_tokens = []\n",
    "    # then loop over each token of the current doc\n",
    "    for token in doc:\n",
    "        # filter out tokens that are pure whitespace\n",
    "        if not token.is_space:\n",
    "            # append a string that matches the AntConc input format\n",
    "            tagged_tokens.append(# please add code here)\n",
    "    \n",
    "    print(\" \".join(tagged_tokens) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa5b1db",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "**6\\. Last task to go for this assignment! Please calculate the token frequencies again. <br/>\n",
    "You can use a for loop like the one in the code block above to append your list with tokens.<br/> After appending your tokens list you can use the Counter module to calculate the frequencies (as in task 2, step 6). (5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e8458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "# please add code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae57bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
